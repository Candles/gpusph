= GPUSPH User Guide
:toc: left
:doctype: book
include::defines.adoc[]
:sectnums:
v{version}

Welcome to GPUSPH, the first implementation of weakly-compressible Smoothed Particle Hydrodynamics
to run entirely on GPU.
The document you are reading is the User Guide for GPUSPH.
The information contained herein will help you navigate the many features of the engine,
through a selection of sample test cases (included in the distribution),
and detailed instructions on how to set up your own.
This document assumes you've already set up a working directory with GPUSPH,
if you haven't, please refer to the xref:installation-guide[Installation Guide]
for instructions on how to set up GPUSPH and its dependencies.

= Introduction

include::common-introduction.adoc[]

== Features

****
#TODO# list or table with features and possibly references (see e.g. SPHERIC 2019)
****

== Building GPUSPH

As explained in the xref:installation-guide[Installation Guide],
each test-case for GPUSPH must be compiled independently,
using
[source,shell,subs="+quotes"]
----
make _ProblemName_
----
where `_ProblemName_` is the name of the test case you want to build, e.g.
[source,shell,subs="+quotes"]
----
make LidDrivenCavity2D
----

Multiple test-cases can be specified at once by specifying their names on the `make` command line.
If a single test-case is built, it becomes the “default” test case, and it can be rebuilt
by simply running `make`. This is very useful during the edit/compiler/run cycle of development.

We also recommend running parallel builds by adding the `-j` \(n\) command line option to `make`
to run \(n\) compile jobs at once. To use all the machine's processors, you can use
[source,shell,subs="+quotes"]
----
make -j$(nproc) _ProblemName1_ _ProblemName2_ _etc_
----

If you run
[source,shell,subs="+quotes"]
----
export MAKEFLAGS=-j$(nproc)
----
then all subsequent invokations of `make` will automatically use all available processors.

=== Built-in documentation and help system

The `Makefile` shipped with GPUSPH provides some built-in documentation that acts as a quick-reference card,
listing the available targets and options without having to consult this manual.

The available documentation targets can be shown with
[source,shell]
----
make help
----

The list of all available test-cases can be shown with
[source,shell]
----
make list-problems
----

The list of all supported targets (including special and documentation targets)
is available through
[source,shell]
----
make help-targets
----

The list of all <<Configuration,compile-time options>> is available through
[source,shell]
----
make help-options
----

The list of configuration overrides that can be set in `Makefile.local` is available through
[source,shell]
----
make help-overrides
----

=== Configuration

Several aspects of GPUSPH can be configured at build time.
These cover things such as the backend (CUDA or CPU),
enabling or disabling features (provided the corresponding headers can be found),
overriding the device architecture,
or some internal details such as the linearization order for the auxiliary grid.

Running
[source,shell]
----
make show
----
will show the current configuration.

[NOTE]
====
If you need to {url-github}issues/[report an issue],
it's a good idea to attach the output of `make show` to your bug report.
You can save the output of the command to disk, together with any warnings or errors
[source,shell]
----
make show > make-show.log 2>&
----
and then attach `make-show.log` to your report.
====

Build-time configuration options can be changed by adding `option=value` pairs to the `make` command line.
These can be specified together with any test case name and other `make` options.
For example, a parallel build of the `DamBreak3D` and `LidDrivenCavity2D` test cases
with the CPU backend and OpenMP support can be launched with
[source,shell]
----
make -j$(nproc) backend=cpu openmp=1 DamBreak3D LidDrivenCavity2D
----

CAUTION: a feature can only be enabled if the corresponding xref:installation-guide#optional[dependencies]
can be found by the build process.
Please refer to the xref:installation-guide[Installation and Quick Start Guide] for additional details.

Option values are persistent across `make` invocations,
so once an option is set (by the user, or through the build system auto-detection),
all subsequent calls to `make` will remember the value,
until a new one is specified or the user runs a `make confclean`.

****
#TODO# extended versions of the Build options section from the installation and quick start guide,
possibly autogenerated?
****

== Running GPUSPH [[run-singlenode]]

Building a problem will produce an executable with the name of the test case.
When building a single test-case, for historical reasons,
a symlink to it will also be created, named `GPUSPH`.
Either of them can be run to execute the test case, e.g.
[source,shell]
----
./LidDrivenCavity2D
----
or
[source,shell]
----
./GPUSPH
----

In the rest of the manual we will use `GPUSPH` as sample executable when referring to a generic test case,
but we highly recommend using the `_ProblemName_` of the specific executable in your daily usage.

Several aspects of the executions of GPUSPH can be controlled at runtime through appropriate
command-line options, documented in <<options,the next section>>.
Test cases can also define their own options.
For the <<Sample test cases>> shipped with GPUSPH, the additional supported options are documented
in the corresponding sections of this manual.

== Command-line options [[options]]

****
#TODO# provisionally these are entered and updated manually.
Ideally we should have a unified place where command-line options are defined and documented,
similar to how we do it for debug flags.
The description should have both a long and a short form, with the short form used for the help text,
and the long form used for the documentation here.
****

GPUSPH supports the following command-line options:

Management::
+
--version::: show the version of GPUSPH (including build options and test case name), and exit;
--help::: show a summary of the command-line syntax with a recap of the supported options, and exit;
--dir _path_::: use the specified _path_ as output directory to write the result files;
the default is `tests/_ProblemName_‍_‍_date_‍T‍_time_` where `_ProblemName_` is the name of the test case,
and `_date_` and `_time_` are the date and time when the simulation was started
(e.g. `tests/DamBreak3D_2021-10-20T08h57`)
--nosave::: do not save intermediate result files (only write the initial and final state of the particle system);
--note _text_::: add a `note.txt` file in the output directory with the given text in it;
this can be used to annotate the simulation with additional information that may be used to identify it at a later time;
--device _spec_::: use the specified device(s) (GPUs, or CPU cores) to run the simulation;
the specification _spec_ can be either a single device, or a range of devices,
or a comma-separated list of device(s); if more than one device gets specified,
multi-GPU (for the CUDA backend) or multi-core (for the CPU backend without OpenMP) is automatically enabled;
examples:
+
[horizontal]
--device 2:::: runs the simulation on device #2;
--device 2-5:::: runs the simulations on devices #2, #3, #4 and #5 (multi-GPU)
--device 2,5:::: runs the simulations on devices #2 and #5 (multi-GPU)
--device 0,2-4:::: runs the simulations on devices #0, #2, #3 and #4 (multi-GPU)

Simulation options::
+
--deltap _f_::: sets the inter-particle distance for the initial filling;
--tend _f_::: sets the end-of-simulation time in (simulated) seconds;
--maxiter _n_::: run the simulation for _n_ iterations only;
--dt _f_::: fix the time-step to _f_ (even if the test-case is configured for dynamic time-stepping);
--dem _path_::: load the Digital Elevation Model (DEM) from the given path (if the test case supports it);
--ccsph-min-det _f_::: sets the CCSPH minimum determinant threshold to the value _f_;
this is ignored unless the test-case enables Conservative Corrected SPH through the `ENABLE_CCSPH` simulation flag;

Checkpointing and resuming::
+
--checkpoint-every _f_::: create checkpoints (hot-start files) every _s_ seconds of simulated time;
these can be used to resume a simulation (see `--resume`) if it terminates for any reason;
the `HotWriter` writer (see <<writers>>) is active by default,
this option can be used to set a different saving frequency than the default or problem-set one;
--checkpoints _n_::: keep the last _n_ checkpoints (older ones will be deleted);
--resume _path_::: resume a simulation from a hot-start file;

[[mgpu-options]]Multi-GPU / multi-node::
+
--striping::: enable overlapping computations with data transers;
this helps reduce latency and improves scaling;
the benefits are more visible on larger simulations and with a higher number of devices;
--num-hosts _n_::: specify number of hosts; this is needed when more than one process
is running on the same host;
--byslot-scheduling::: sets the MPI scheduler to fill hosts first, instead of using the default
round-robin scheduling;
--gpudirect::: enable GPUDirect for RDMA;
this option allows GPUs from different nodes to communicate directly through the network
without involving the hosts;
requires a CUDA-aware MPI library (see <<run-multinode>>);
--asyncmpi::: enables asynchronous network transfers; requires GPUDirect (see `--gpudirect` above)
and one process per device (see <<run-multinode>> for further details);

Debugging options::
+
--no-leak-warning::: do not warn if the number of particles decreases during the simulation
in absence of open boundaries (i.e. due to overtopping or other leakage);
--debug _flags_::: enable one of several debugging options (see <<Debug options>>);
--pin-fea-buffers, --no-pin-fea-buffers::: controls whether the buffers used for host-device data exchange
with FEA should be pinned or not;

Repacking*::
+
--repack::: run the repacking step before the simulation;
needs a test case for which repacking has been enabled through the `ENABLE_REPACKING` simulation flag;
--repack-only::: run the repacking step only; do not run the simulation afterwards;
--repack-maxiter _n_::: maximum number of iterations for the repacking step;
--from-repack _path_::: start the simulation from a previous repack file;

Visualization*::
+
--display::: enable co-processing visualization by integrating with ParaView Catalyst;
--display-every _f_::: output simulation data for visualization every _f_ seconds of simulated time;
--display-script _path_::: path to the co-processing Python script;

Options marked with a `*` refer to experimental features, and may change or be removed in the future.

=== Debug options

GPUSPH provides a number of tracing options for the simulator that provide insights
on the evolution of key internal structures during initialization and execution.
These tracing options can be of use to both developers and end-users to determine
(and possibly fix) issues with the code or with the test-case setup.

Tracing options are enabled through the `--debug` command-line parameter,
as a comma-separated list of features; e.g.
[code,shell]
----
./GPUSPH --debug validate_init_positions,nans,sync_kernels
----
can be used to ensure no particles are placed out-of-bounds during initialization,
to be informed of any invalid position generated during integration,
and to force synchronization between host and device after each computational kernel.

****
#TODO# generate list from `src/debugflags.def`
****

The available tracing options are:
[horizontal]
print_step:: print each (sub)step of the system as it is being executed;
neibs:: debug the neighbors list on host;
buildneibs_by_particle:: force the neighbors list construction to map work-items to particles;
the default on GPU is to map work-groups to cells instead;
forces:: debug forces on host;
cspm:: debug CSPM on host;
numerical_density:: debug relative density variation on host;
inspect_preforce:: inspect pre-force particle status;
inspect_pregamma:: inspect post-Euler, pre-gamma integration particle status;
inspect_buffer_access:: inspect buffer access;
inspect_buffer_lists:: inspect buffer lists at the end of each command;
check_buffer_update:: check that all buffers are properly updates in multi-GPU mode;
check_buffer_consistency:: check buffer consistency between GPUs;
clobber_invalid_buffers:: initialize invalid buffers with invalid data to verify that they are not being used when they shouldn't;
validate_init_positions:: raise an exception (instead of just a warning) if a particle is out of bounds during initialization of the particle system;
check_cell_overflow:: check (and raise an exception) if cells have more particles that can be encoded in the compact neighbors list;
planes:: warn when particles end up behind geometric planes or DEMs;
validate_roll_call:: raise an exception (instead of just a warning) if there are issues at roll call;
benchmark_command_runtimes:: measure (and show) the runtimes of each executed command;
nans:: check for NaNs after integration, and abort if found;
sync_kernels:: make kernel launches synchronous;
this ensures that any errors raised by a kernel will be signaled at the right place,
instead of at the next synchronization point between host and device.

== Running multi-node simulations [[run-multinode]]

GPUSPH can distribute the computation of a simulation on multiple
GPU devices attached to different nodes of a cluster in different ways.

Say we want to launch a simulation on \(N\) nodes, each with \(D\) devices
(with CUDA device numbers ranging from \(0\) to \(L = D-1\) inclusive);
the total number of devices in the simulation will be \(N \times D\).
We can run either:
. one process per node, \(D\) GPUs per process
. \(2\) processes per node, \(D/2\) GPUs per process
. \(4\) processes per node, \(D/4\) GPUs per process
. etc.

Additionally, some MPI implementations have built-in support for CUDA,
which allows for faster communication between devices on different nodes.
Experimental support for this feature can be enabled in GPUSPH with the `--gpudirect` <<mgpu-options,command-line option>>.

The best decision on how to distribute the computation across nodes and devices depends on
the queue policy of the cluster, on the network topology, on simple a posteriori performance tests,
on the capabilities of the MPI implementation, etc.

=== One process per node

If we wanted to run the simulation on all the devices of one node, we would run in an interactive shell,
using
[source,shell]
----
./GPUSPH --device 0-L
----

Running the same simulation on multiple nodes only requires to run
the same command within the reference MPI launcher
(usually a script called `mpirun`);
GPUSPH will retrieve the necessary information about the launch environment
directly from the MPI runtime and will organize the node-to-node communication accordingly:
[source,shell]
----
mpirun -np N ./GPUSPH --device 0-L
----

This command leaves to MPI the choice of which nodes to use in the network,
if more than N are available.
It is always safe to provide MPI a list of hostnames corresponding
to the nodes chosen to run the simulation.
If the file containing the list of hostnames is called `myhostsfile`, a typical syntax will be:
[source,shell]
----
mpirun -np N -hostfile ./myhostsfile ./GPUSPH --device 0-L
----

Please note the syntax may vary from one MPI library to another.
For example, MVAPICH uses `-hostfile` while OpenMPI `--hostfile`.

=== Multiple processes per nodes

Let us now see the command line options needed to run the same simulation
with more processes (and thus on more nodes) and a smaller number of devices per process.
In this case we need to inform both the MPI runtime and GPUSPH.
For the former, we simply decrease the number of processes to start (with the `-np` option);
for the latter, we need to shorten appropriately the list of devices 
passed with the `--device` option.

If our aim is to run \(2N\) processes each using \(D/2\) devices, we then run:
[source,shell]
----
mpirun -np N*2 -hostfile ./myhostsfile ./GPUSPH --device 0-L2
----
where \(L_2 = D/2 - 1\)

Here we need to take care of a few important details.
If the list of available hosts contains at least \(2N\) hostnames,
the MPI runtime will start every process on a different physical node.
But what happens if the list is shorter (e.g. \(N\) hosts only),
or if we want anyway to use a smaller number of nodes,
for example because part of the cluster is already used by other processes?

The MPI runtime will start multiple processes per node and the GPU device
numbers will be likely to conflict (i.e. two different processes might
try to use the same GPU device for different parts of the simulation domain,
causing a performance slowdown, or a failure if the CUDA devices are set
in ``exclusive mode'').

In this case, we must inform GPUSPH that the number of physical hosts (i.e. nodes)
is smaller than the number of processes, so that it will shift the GPU device
numbers of the appropriate processes and no GPU device will be accessed by two processes.
The corresponding option is `--num-hosts`:
[source,shell]
----
mpirun -np N*2 -hostfile ./myhostsfile ./GPUSPH  --num-hosts N --device 0-L2
----

But there is another important detail. There are different ways the MPI
runtime can distribute the processes across the nodes.
Two very common policies are ``by slot'' (fill-first) and ``by node''
(round robin).
The scheduling policy affects the association between the process ranks
and the CUDA device numbers, so GPUSPH must be informed about it to use
the appropriate offsets.
GPUSPH assumes a round robin schedule is being used;
if this is not true, the `--byslot-scheduling` <<mgpu-options,option>> must be passed:
[source,shell]
----
mpirun -np N*2 -hostfile ./myhostsfile ./GPUSPH --num-hosts N --byslot-scheduling --device 0-L2
----

There is no optimal policy in general as its performance depends on the node
load and the node-to-node connection speed. It is worth trying both to check
whether one is more performant than the other.
The default policy is usually ``by slot''
(fill-first, usually preferred by non GPU-based software)
but it is always safer to explicitly set it for every run.
In OpenMPI the corresponding options for `mpirun` are `--byslot` and `--bynode`.

One final possibility is to run one process per device. This is the most
consuming option from the point of view of the host memory, since every process
will allocate on host the whole simulation scenario, but it might be useful if the
GPUDirect feature is being used and the MPI runtime does not support multiple
devices per process, or if the amount of data to be saved on file is large or
the saving frequency is very high, so that saving would benefit from a parallel dump
(every process saves its part of the simulation independently from the other processes).

=== Examples

Finally, let's see some practical examples. Suppose our cluster has 12 nodes,
each equipped with 4 GPU devices.
We need to run a simulation on 12 devices.
To run 3 processes on 3 hosts, each using 4 devices, we will run:
[source,shell]
----
mpirun -np 3 -hostfile ./myhostsfile ./GPUSPH --device 0-3
----

To run 6 processes on 6 hosts, each using 2 devices, we will run:
[source,shell]
----
mpirun -np 6 -hostfile ./myhostsfile ./GPUSPH --device 0,1
----

Note that another simulation can be run simultaneously on the remaining 2 devices of the same nodes, with:
[source,shell]
----
mpirun -np 6 -hostfile ./myhostsfile ./GPUSPH --device 2,3
----

To run 6 processes on 3 hosts, each using 2 devices, we will run:
[source,shell]
----
mpirun -np 6 -hostfile ./myhostsfile_with3hosts ./GPUSPH --num-hosts 3 --device 0,1
----
(`--byslot-scheduling` might also be necessary).

To run 12 processes on 12 hosts, each using 1 device, we will run:
[source,shell]
----
mpirun -np 12 -hostfile ./myhostsfile ./GPUSPH --device 0
----

Note that another simulation can be run simultaneously on one of the free devices of each node (e.g. device number 3), with:
[source,shell]
----
mpirun -np 12 -hostfile ./myhostsfile ./GPUSPH --device 3
----

Please note the options for the MPI library always precede the GPUSPH executable name.
If the MPI library supports it, we also suggest enabling the option to tag each
line of the output with the process rank that has generated it;
in OpenMPI, the option is called `--tag-output` while in MVAPICH `-prepend-rank`.
This will come very helpful when the logs need to be analyzed

TIP: use `grep` to separate the logs if they are multiplexed.

If you need to use any queue-management system, remember to inform it about the desired topology,
coherently with the options passed to GPUSPH.
For example, with PBS you would set the `nodes` and `ppn` parameters for the number of hosts and processes per host,
respectively.

= Sample test cases

GPUSPH ships by default with a number of sample test cases, covering most of the features of the engine.
You can study their source code to see how things are done, and use them as basis for your own test cases.
Of course, you can also write your test cases from scratch,
following the instructions in <<byotc,the dedicated part of the User Guide>>.

This part of the User Guide illustrates in detail a choice of the default test cases,
describing the associated theoretical and numerical model, the GPUSPH features involved,
the supported options, and the expected results.

****
#TODO# look into a way to autogenerate these,
possibly from source file comments
****

include::user-guide/test-case-DamBreak3D.adoc[]

include::user-guide/test-case-DamBreakGate.adoc[]

include::user-guide/test-case-OpenChannel.adoc[]

include::user-guide/test-case-WaveTank.adoc[]

include::user-guide/test-case-SolitaryWave.adoc[]

include::user-guide/test-case-Seiche.adoc[]

include::user-guide/test-case-DEMExample.adoc[]

****
#TODO# import other test cases description from the old LaTeX documentation
****

= BYOTC (Build Your Own Test Case) [[byotc]]

Test cases in GPUSPH are subclasses of the `Problem` class,
defined in source files matching the test case name and placed under the `src/problems/` directory.
Users are encouraged to put their test cases under `src/problems/user/` to keep them separate from
the <<Sample test cases>>.

For example, to create a `MyCase` test case, one would create
`src/problems/user/MyCase.h` containing the declaration of the `MyCase` class,
and a `src/problems/user/MyCase.cu` containing its implementation, with the definition
of the geometry, simulation settings, physical parameters, etc.

<<chrono-v-cuda,Under some circumstances>>, it may be necessary to split the implementation
into separate host and device parts, see <<chrono-v-cuda,the relevant section>> for further details.

As a general rule, a test case only needs to define a constructor
(e.g. `MyCase::MyCase`), that <<setup-framework,sets up the simulation framework>>,
defines any needed <<sim-params,simulation parameters>>,
<<phys-params,physical parameters>> and <<materials, material properties>>,
defines the domain <<geometries>> and chooses <<writers,how to output the data>>.

In some cases, it may be useful or necessary to define some <<aux-funcs,auxiliary functions>>
to take care of things such as moving bodies or custom particle initialization.

include::user-guide/reference-manual.adoc[]

== Chrono and CUDA conflicts [[chrono-v-cuda]]

Some versions of Chrono can cause compilation issues when both host-code using Chrono
and device-code (not using Chrono) are compiled as part of the same compilation units.
Common reasons for this are the use of a host compiler (version) not fully supported by the CUDA compiler,
or by limitations in the support of CUDA in some versions of the Eigen library.

The simplest solution to these issues is to separate the host and device code for your test case,
by creating both a `.cc` file (containing only host code, including any usage of Chrono)
and a `.cu` file (containing the device code, without any references to Chrono).

In such a case, the `.cu` file will usually include only `cudasimframework.cu` and a function
that wraps the call to `SETUP_FRAMEWORK`.
The `FullPlatform` test case provides an example on how to achieve this.

****
#TODO# Example in line with Vito's single-source example
****


= Appendix
:appendix-caption!:

[appendix]
include::user-guide/appendix-history.adoc[]

[appendix]
include::appendix-license.adoc[]

bibliography:[]
